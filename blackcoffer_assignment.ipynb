{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpELzcY3wzm8",
        "outputId": "0d5770ab-9499-4ba8-8fbf-5e95f48181bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeTG6FeJWo_q"
      },
      "source": [
        "TEXT EXTRACTED AND SAVED INTO FILES. FILES ARE IN A DIRECTORY NAMED 'ARTICLES'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhXPTTAegx6R"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import spacy\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import nltk\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5-Aw1Z_-Wjge"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_text_from_url(url):             # Function to extract article text from URL\n",
        "    try:\n",
        "        response = requests.get(url)          # Fetch HTML content from URL\n",
        "        html_content = response.text\n",
        "\n",
        "        soup = BeautifulSoup(html_content, \"html.parser\")   # Parse HTML using BeautifulSoup\n",
        "\n",
        "        article_title = soup.title.get_text() if soup.title else \"\"\n",
        "\n",
        "        doc = nlp(html_content)            # Process HTML content\n",
        "\n",
        "        article_text = \"\"                                    # Extract text\n",
        "        for p in soup.find_all('p'):\n",
        "            article_text += p.text + \"\\n\"\n",
        "\n",
        "        return article_title, article_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching or parsing article from {url}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "input_df = pd.read_excel(\"Input.xlsx\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzCYtXy5g4kE"
      },
      "outputs": [],
      "source": [
        "\n",
        "if not os.path.exists(\"articles\"):\n",
        "    os.makedirs(\"articles\")\n",
        "\n",
        "for index, row in input_df.iterrows():             # Extract article text for each URL and save into text files\n",
        "    url_id = row[\"URL_ID\"]\n",
        "    url = row[\"URL\"]\n",
        "    article_title, article_text = extract_text_from_url(url)\n",
        "    if article_title and article_text:\n",
        "        with open(f\"articles/{url_id}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(f\"{article_title}\\n\\n{article_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFKjpIMqxtWy"
      },
      "source": [
        "# **Data Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcTkzE_YPUnd"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):                                        # Function to clean text\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    words = [word.lower() for word in text.split('.') if word.strip() and word.lower() not in stopwords and word.strip() not in string.punctuation]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Load stop words\n",
        "stopwords_files = [\"StopWords_Auditor.txt\", \"StopWords_Currencies.txt\", \"StopWords_DatesandNumbers.txt\", \"StopWords_Generic.txt\", \"StopWords_GenericLong.txt\", \"StopWords_Geographic.txt\", \"StopWords_Names.txt\"]\n",
        "stopwords = set()\n",
        "for file in stopwords_files:\n",
        "    with open(file, 'r', encoding='latin-1') as f:\n",
        "        stopwords.update(f.read().splitlines())\n",
        "\n",
        "# Process each file in the articles directory\n",
        "articles_dir = \"articles\"\n",
        "output_dir = \"cleaned_articles\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b_VhvDhhimO"
      },
      "outputs": [],
      "source": [
        "for filename in os.listdir(articles_dir):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        # Read the text file\n",
        "        file_path = os.path.join(articles_dir, filename)\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:  # Specify encoding as 'utf-8'\n",
        "            text = f.read()\n",
        "\n",
        "        # Clean the text\n",
        "        cleaned_text = clean_text(text)\n",
        "\n",
        "        # Write cleaned text to a new file in the output directory\n",
        "        output_file_path = os.path.join(output_dir, filename)\n",
        "        with open(output_file_path, 'w', encoding='utf-8') as f:  # Specify encoding as 'utf-8'\n",
        "            f.write(cleaned_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxVRhhmaSkl0"
      },
      "source": [
        "# **TOKENIZATION AND VECTORING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "rRjIwuYpSl1n"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import cmudict\n",
        "\n",
        "# Load positive and negative words\n",
        "positive_words_file = \"positive-words.txt\"\n",
        "negative_words_file = \"negative-words.txt\"\n",
        "\n",
        "with open(positive_words_file, 'r', encoding='latin-1') as f:\n",
        "    positive_words = set(f.read().splitlines())\n",
        "\n",
        "with open(negative_words_file, 'r', encoding='latin-1') as f:\n",
        "    negative_words = set(f.read().splitlines())\n",
        "\n",
        "# Load CMU Pronouncing Dictionary for syllable count\n",
        "cmu_dict = cmudict.dict()\n",
        "\n",
        "# Function to count syllables in a word\n",
        "def count_syllables(word):\n",
        "    if word.lower() in cmu_dict:\n",
        "        return len([ph for ph in cmu_dict[word.lower()] if ph[-1].isdigit()])\n",
        "    # Handling exceptions for words ending with \"es\" or \"ed\"\n",
        "    elif word.lower().endswith(('es', 'ed')) and word.lower()[:-2] in cmu_dict:\n",
        "        return len([ph for ph in cmu_dict[word.lower()[:-2]] if ph[-1].isdigit()])\n",
        "    else:\n",
        "        return len([ph for ph in cmu_dict.get(word.lower(), []) if ph and ph[-1].isdigit()])\n",
        "\n",
        "# Function to calculate readability metrics\n",
        "def calculate_readability(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = word_tokenize(text)\n",
        "    total_words = len(words)\n",
        "    total_sentences = len(sentences)\n",
        "    total_syllables = sum(count_syllables(word) for word in words)\n",
        "    complex_word_count = sum(1 for word in words if count_syllables(word) > 2)  # Count words with more than 2 syllables\n",
        "\n",
        "    avg_sentence_length = total_words / total_sentences\n",
        "    percentage_complex_words = (complex_word_count / total_words) * 100 if total_words > 0 else 0  # Calculate percentage of complex words\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "    avg_words_per_sentence = total_words / total_sentences\n",
        "    avg_word_length = sum(len(word) for word in words) / total_words  # Calculate average word length\n",
        "\n",
        "    return avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence, complex_word_count, total_words, total_syllables, avg_word_length\n",
        "\n",
        "# Function to perform sentimental analysis\n",
        "def perform_sentimental_analysis(text):\n",
        "    words = word_tokenize(text.lower())\n",
        "    positive_score = sum(1 for word in words if word in positive_words)\n",
        "    negative_score = sum(1 for word in words if word in negative_words)\n",
        "    polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)\n",
        "    subjectivity_score = (positive_score + negative_score) / (len(words) + 0.000001)\n",
        "\n",
        "    return positive_score, negative_score, polarity_score, subjectivity_score\n",
        "\n",
        "# Process each file in the articles directory\n",
        "articles_dir = \"articles\"\n",
        "output_data = []\n",
        "\n",
        "for file in os.listdir(articles_dir):\n",
        "    if file.endswith(\".txt\"):  # Check if the file is a text file\n",
        "        with open(os.path.join(articles_dir, file), 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "            avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence, complex_word_count, total_words, total_syllables, avg_word_length = calculate_readability(text)\n",
        "            positive_score, negative_score, polarity_score, subjectivity_score = perform_sentimental_analysis(text)\n",
        "            output_data.append([positive_score, negative_score, polarity_score, subjectivity_score,\n",
        "                                avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence,\n",
        "                                complex_word_count, total_words, total_syllables, avg_word_length])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56aO4fdshsfF"
      },
      "outputs": [],
      "source": [
        "output_df = pd.DataFrame(output_data, columns=[\n",
        "    \"POSITIVE SCORE\", \"NEGATIVE SCORE\", \"POLARITY SCORE\", \"SUBJECTIVITY SCORE\",\n",
        "    \"AVG SENTENCE LENGTH\", \"PERCENTAGE OF COMPLEX WORDS\", \"FOG INDEX\",\n",
        "    \"AVG NUMBER OF WORDS PER SENTENCE\", \"COMPLEX WORD COUNT\", \"WORD COUNT\",\n",
        "    \"SYLLABLE PER WORD\", \"AVG WORD LENGTH\"\n",
        "])\n",
        "\n",
        "output_df.to_excel(\"output.xlsx\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "17_45-nun6Ea"
      },
      "outputs": [],
      "source": [
        "\n",
        "output_df.insert(0, \"URL_ID\", input_df[\"URL_ID\"])\n",
        "output_df.insert(1, \"URL\", input_df[\"URL\"])\n",
        "\n",
        "output_df.to_excel(\"output.xlsx\", index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
